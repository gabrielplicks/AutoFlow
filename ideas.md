# AutoFlow vs AFlow

Important point: they generate ONE workflow to evaluate against a whole dataset! Example: generate one workflow that will perform well in the HotpotQA dataset.

## AutoFlow

### Workflow representation

- Workflows in AutoFlow are represented as NL structure with syntax like:
  - Step Name: A unique identifier for each step.
  - Step Type: Defines the instruction type (Process, Decision, Terminal). [cite: 68, 69, 70, 71]
  - Step Instruction: A natural language instruction to be executed. [cite: 72]
  - Step Connection: Points to the next step, establishing the flow. [cite: 73]

#### Example

Step 1::: Process ::: Identify the input data type based on the objective .::: next :: Step 2
Step 2::: Process ::: Identify the output data type based on the objective .::: next :: Step 3
Step 3::: Process ::: Select tools in the provided tool list to generate a plan .::: next :: Step 4
Step 4::: Decision ::: Check whether every tool in the plan is in the provided tool list .::: Yes :: Step 5:: No :: Step 3
Step 5::: Decision ::: Check whether the output data type of the previous tool is the input data type of the next tool .::: Yes :: Step 6:: No :: Step 3
Step 6::: Terminal ::: Output the plan by listing the tool names .:::

### Training

AutoFlow proposes two main methods for workflow generation, one with fine-tuning and another with in-context learning. Both work like this at their core:

- A **Workflow Generator LLM** (e.g., GPT-4 for in-context, Mixtral for fine-tuning) is given a few-shot example of NL workflows and a description of the task.
  - Because some models might struggle to generate grammatically perfect CoRE (their language) workflows, GPT-4 is used to revise the generated workflow and ensure it's valid.
- A separate, frozen **Workflow Interpreter LLM** executes this generated workflow on a validation dataset.
  - Workflow execution here is an LLM-centred process, where each step of the workflow will be sent to an LLM that checks if it should make a tool call (and with what data) or answer directly, etc, and so on until the workflow is finished.
- The performance of the executed workflow on the validation dataset serves as the reward signal.
- Optimisation step:
  - For **in-context** learning: the performance score from the previous workflow is directly incorporated into the prompt for the next iteration. The LLM is prompted to generate a *new* workflow that aims for better performance, given the feedback on the previous one (e.g., "The previous workflow scored 0.6415. Provide a new one that performs better.").
  - For **fine-tuning**: the paper mentions REINFORCE, but in code they're using PPO with the collected experiences of prompt, response (the workflow), and reward to update the LLM's parameters
- Iterate keeping chat history

---

## AFLOW

### Workflow representation

They represent workflows as Python code, where some nodes (operators) can be called. Nodes are LLM calls and are predefined, e.g., Ensemble, Review & Revise, Generate, Format, Test, Programmer, and Custom (for a custom prompt I guess).

#### Example

A Workflow class with a `__call__` method generated by the LLM:

```python
async def __call__(self, problem: str):
    """
    Implementation of the workflow
    """
    solutions = []
    for _ in range(3):
        initial_response = await self.answer_generate(input=problem)
        thought_process = initial_response['thought']
        initial_answer = initial_response['answer']
        solutions.append(initial_answer)
        
    ensemble_result = await self.sc_ensemble(solutions=solutions)
    best_answer = ensemble_result['response']

    refined_solution = await self.custom(
        input=f"Question: {problem}\nBest answer: {best_answer}",
        instruction=prompt_custom.FORMAT_ANSWER_PROMPT,
    )

    return refined_solution['response'], self.llm.cost_manager.total_cost
```

### Training

AFLOW uses MCTS (but not real MCTS at its core) as its core framework to explore possible workflows. The process is iterative:

- Start with an **initial template workflow**.
- The algorithm **selects an existing workflow from the tree** to expand upon (with a "soft mixed probability selection strategy")
- An **LLM acts as an optimizer**, taking the selected workflow and its associated "experience" (past modifications, performance, logs) and generates a *new* workflow by modifying the code of the selected workflow (this could be changing prompts within nodes or altering the code-based edges that connect nodes).
- The newly generated workflow is executed on the validation dataset. The performance is recorded and associated to this workflow as its score (or reward if you will).

This cycle continues until a termination condition is met, such as a maximum number of iterations or if the average score of the top-k workflows stops improving for a set number of rounds.



# Tools

n8n Expert Agent Workflow: https://github.com/MuLIAICHI/n8n-python-integration?tab=readme-ov-file
DeepEval synthesizer (goldens): https://deepeval.com/docs/synthesizer-generate-from-scratch 




Here's the situation: I need to create a tool/algorithm that will automatically generate n8n workflows (see examples in attachment) given a description in natural language. There are a few things in the literature that propose some ways to generate workflows, see them in attachment. I need your ideas on how to get this done.

We have:
- A dataset with ~2k workflows + their descriptions
  - How to use this dataset:
    - Fine-tune an LLM
    - Do RAG for every query (I want to create an agent that does this and this and that...)
    - High-quality few-shot examples
    - Hybrid (all of the above)
  - Another complementary dataset possibility: the n8n docs to add proficiency to the LLM

We don't have:
- A proper optimisation algorithm to work with the space of n8n nodes and connections to automatically generate workflows
  - The n8n workflows are quite complex, their nodes and connections have parameters and configurations. But here are some ideas:
    - Reduce complexity of nodes structure (to reduce search space) to just their names, and leave to a final refinement process the burden of generating the arguments/parameters (once we already have the backbone of the workflow)
    - Work on a subset of possible nodes connections
  - Should we work with reinforcement learning, genetic algorithms? What other possible algorithms/frameworks?
    - If RL or GA, how would that work in technical details/algorithmically? How to evaluate and give a fitness/reward metric? What are the possible mutations/actions? 
    - How to prepare our dataset to generate the set of actions? What should be the actions? How do we lay out the possible actions to the LLM? How do we make everything fit in the context window?
- Proper ways to test and evaluate a workflow both syntatically and semantically, so we don't know if all necessary nodes are correctly placed and the middle steps correctly do what they have to do (not only the final output verification)
  - Our dataset doesn't help here, cause each item in the dataset is a specialised workflow
  - We don't have expected set of inputs/outputs for each workflow
  - There's no principled/grounded approach to execute the workflow and get a number/score that represents its quality

---

Here are my thoughts:

Let's start engineering a solution that is a quick POC for now!
- Let's use in-context learning, so no need to do RAG or fine-tuning for now
- Let's use an LLM to evaluate workflow quality (or "LLM-as-a-Judge" if you will call it like that)
- Let's use MCTS for iterative refinement (with actions to add/remove nodes and connections)
  - Should our take on MCTS store visit counts and value function? How are we going to keep our tree representation for the LLM? Should we even store a value function, or the LLM is capable enough of implicitly "traversing the tree" and selecting which workflow should be expanded? Should we add an action such as "workflow finished" so that only then we allow for evaluation?
  - One of the questions here is should we actually run MCTS or should we give it all to the LLM to take the decisions based on the current explored workflows?
- Should every action also allow setting the node parameters? Should we have actions for setting/updating parameters of nodes?
  - It is important to note that some workflows contain nodes that run code (e.g., shell, python), and this code will have to be written by the LLM (at some point, i.e. at creation time, at parameter update time, or at a final refinement if we decide to leave parameters behind during MCTS time). Notice that code is a node parameter, so we might consider the LLM writing it at creation, parameter update, and maybe not need a dedicated action to it
- How many LLMs should we have? I was thinking perhaps having a powerful model to interpret the instructions at first and generate an abstract plan, then we might use the same (?) LLM to generate the first workflow (initial state, "warm-started" by few-shot examples as well). Then I was thinking of having a cheaper LLM model to run expansions and deal with all the context window that will grow, and a third LLM to be the judge. What do you think? Or should 2 LLMs be fine? Or just 1?
  - And also very importantly, what things should be in the context (messages history) of each LLM call?
- After some thoughts, this iterative refinement approach is less of a pure search algorithm like MCTS and more of an agentic loop where the LLM is the primary driver of decisions. The question is, how many workflows and history of alterations should the LLM be aware of at each call? How do we structure the prompt? 

Make a critical analysis of my thoughts and ideas so we can think of some solution together.


---

I've decided to not have an action to mark a workflow as "final" and ready to judge, I'll just assume all workflows are proper instantiatations

Important: at some point we might converge to all workflows being equivalent?? We might need to check this

Algorithm:

Dataset cleaning:

- Remove the "link" key (useless)
- Fields in the `workflow_json`:
  - For "meta"
    - remove `instanceId`, `templateCredsSetupCompleted`
  - For "nodes":
    - remove `id`, `webhookId`, `typeVersion`
    - remove credentials?
    - UI-related:
      - `position`
  - Remove "pinData" key, "active", "versionId", "settings"
  - For nodes with credentials (WhatsApp, Google API, etc)
    - put a `"PLACEHOLDER"` string
  - Remove sticky notes?
- RUN JSON MINIFIER

- Init:
  - Sampler LLM (to warm-start the search set)
    - Purpose: here we will create a set of workflows (output them either all in one call or in multiple calls)
    - Input:
      - NL task description (what the workflow should accomplish)
      - Few-shot examples
      - Instruction to generate K workflows in JSON format, encouraging variety in approaches if possible
    - Output:
      - K workflows in JSON format
  - Candidate Set Manager
    - Purpose:
      - We might need to discard some workflows from time to time so we don't overflow the LLM's context window
      - Maintains the data structure comprising Workflow ID, Workflow JSON, latest feedback, latest score, and other necessary stats
        - Maybe maintain history of changes applied to it with the corresponding scores as well (although not trivial to manage?)
    - Data structure:
      - Workflow ID
      - Workflow JSON
      - Latest Semantic Score (from Judge LLM)
      - Latest Semantic Feedback (from Judge LLM)
      - Number of times selected/modified (iteration count for this candidate)
      - Parent Workflow ID (an easy way to track history/lineage)
      - Actions applied to Parent ID (optional): summary of what actions transformed the parent into this candidate to help the Decision-Maker understand evolution

- Loop (iterate a predefined number of times or until a stopping condition is met):
  - Decision-Maker LLM
    - Purpose: this is where one workflow gets selected from the set and changes are proposed (LLM can suggest a sequence of actions to apply, not necessarily one only)
    - Input:
      - NL task description (what the workflow should accomplish)
      - Current set of candidates
      - Few-shot examples
      - Instruction to pick a workflow (balance exploration and exploitation) and propose actions to modify it
    - Output:
      - Chosen workflow ID to operate on
      - List of (structured) actions to take (add node A with param 1 and 2, add node B with param 3, connect node A to B)
      - Provide reasoning for the selection and the proposed actions
  - Action Executor LLM
    - Purpose: we set this LLM to do the JSON changes alone so it doesn't overloads the Decision-Maker
    - Input:
      - NL task description (what the workflow should accomplish)
      - Few-shot examples (and potentially node docs from n8n)
      - The chosen workflow in JSON from the Decision-Maker LLM
      - The actions to apply
    - Output:
      - Updated JSON workflow (actually a new candidate, and previous one will be assigned as parent of this new one)
  - Syntax Validator (maybe uses LLM)
    - Check if the JSON is structurally valid and conforms to n8n schema (programmatically)
    - We will only store syntactically valid workflows!
    - May trigger an LLM to run fixes in the schema
    - Input:
      - The modified workflow from the Action Executor.
    - Process:
      - Programmatically check if the JSON is well-formed.
      - Programmatically validate against the n8n workflow JSON schema.
        - This includes checking for required fields, correct data types for parameters, valid connection structures, etc.
    - Output:
      - `Valid` or `Error` messages: list of specific syntax errors or traceback from `json.loads` if invalid.
    - NOTE: an LLM might be used in a subsequent "repair" step if validation fails, but the primary validation should be programmatic for speed and accuracy.
  - Semantic Validator/Judge LLM
    - Purpose: to evaluate how well the workflow addresses the original NL task description and to provide qualitative and quantitative feedback.
    - Input:
      - NL task description
      - The Workflow JSON
      - Optional here cause we might introduce bias: previous semantic score and feedback for this workflow's lineage
      - Clear evaluation criteria (e.g., "Rate on a scale of 1-10 for: 1. Relevance to task, 2. Completeness of solution, 3. Logical flow of nodes, 4. Correctness of node choices, 5. Efficiency/Simplicity. Provide specific suggestions for improvement.")
    - Output (JSON):
      - `semantic_score`: (e.g., 1-10 overall, or per criterion)
      - `semantic_feedback`: textual explanation of the score, highlighting strengths, weaknesses, and specific actionable suggestions for improvement
      - `is_solution_satisfactory`: True/False
  - Candidate Set Manager update
    - Purpose: add new workflow and run pruning strategy (e.g., remove lowest-scoring candidates if set exceeds N items, remove candidates not improved after M attempts, etc)
